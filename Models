Gathered some possible LLM to run locally
Some are faster, less powerful, while others are pure power, no speed

Faster models:
  qwen3:8b-q8_0 (qwen3 8 billion parameters, quantized to 8 bit)
  llama3.1:8b(llama 3.1 8 billion parameters)

Pure Power models:
  qwen3:14b (qwen3 14 billion parameters)
  mistral-nemo


For more ideas: ask Gemini

Interesting: finBERT
